{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover the power of Deep Learning\n",
    "\n",
    "In this tutorial, you'll discover how to use \"deep learning\" (DL) to classify digits, ranging from 0 to 9. The dataset is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/. A French guy put it up, very famous in the DL comunity, he's called Yann Lecun and is now both head of the Facebook AI reseach program and head of something in the University of New York (you may want to search and pull the answer :p ).\n",
    "\n",
    "I invite you to discover how MNIST truly is (class distribution, pixels distribution...).  \n",
    "Luckilly for you, I managed to be organised this time, and you may find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) usefull.\n",
    "\n",
    "Remember logistic regression ? I also happen to have a notebook about this [here](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Logistic_regression.ipynb). It's all done with Keras and might help you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape Xs and Ys\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTF are we going to do in this notebook ?\n",
    "Ok, if you are up to this line, I expect you to know what is MNIST, and it's associated classification task. If you didn't got the task you can google something like this : \"what is the classification task of MNSIT\".\n",
    "\n",
    "Perfect. Therefore, we want to classify MNIST. To do so, we'll use a neural network !!\n",
    "The neural net will be as follows :\n",
    "- It takes as input a batch of shape(32, 28 * 28)\n",
    "- Then has 3 * 128 fully connected layers (also called 'Dense layer') with Relu activations.\n",
    "- And finishes with a 10 dimention dense layer (which should be interpreted as probabilities (<=> sums to one))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(28 * 28,), activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Loss definition (from logits)\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN !!\n",
    "Fit your data on the model you've created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 2s - loss: 14.6648 - acc: 0.0901 - val_loss: 14.6522 - val_acc: 0.0909\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 2s - loss: 14.6659 - acc: 0.0901 - val_loss: 14.6522 - val_acc: 0.0909\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 2s - loss: 14.6659 - acc: 0.0901 - val_loss: 14.6522 - val_acc: 0.0909\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 2s - loss: 14.6659 - acc: 0.0901 - val_loss: 14.6522 - val_acc: 0.0909\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 2s - loss: 14.6659 - acc: 0.0901 - val_loss: 14.6522 - val_acc: 0.0909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96e8454a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok... That's bad !\n",
    "We've reached 10% accuracy, which is pure random, it's very bad ! Somehow, I can tell you that your classifier isn't converging, not even to random !! At random, you loss should be $ln(1/10) = 2.3$. By experience, I can tell you that your gradient step is too large...\n",
    "\n",
    "You can try changing the lr parameter in `keras.optimizers.SGD` (yes, try this !) What happens ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR wasn't the true culpable ...\n",
    "The Gradient descent algorithms are quite scaled to normalized dataset... Yet our dataset has a poor distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33.318421449829934, 78.567489983397977)\n"
     ]
    }
   ],
   "source": [
    "x_mean, x_std = x_train.mean(), x_train.std()\n",
    "print(x_mean, x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset such that mean=0 and std=1\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let re-train.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 2s - loss: 0.3480 - acc: 0.9009 - val_loss: 0.1771 - val_acc: 0.9469\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 2s - loss: 0.1493 - acc: 0.9533 - val_loss: 0.1550 - val_acc: 0.9530\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 2s - loss: 0.1105 - acc: 0.9662 - val_loss: 0.1528 - val_acc: 0.9552\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 2s - loss: 0.0918 - acc: 0.9717 - val_loss: 0.1298 - val_acc: 0.9631\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 2s - loss: 0.0741 - acc: 0.9758 - val_loss: 0.1300 - val_acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96e8454bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YEAH !!!\n",
    "Try to beat this !! \n",
    "- you may use something else than SGD\n",
    "- you may regularize the neurons\n",
    "- you may try dropout\n",
    "- you may use batch normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
