{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Shallow Neural Networks\n",
    "\n",
    "In this tutorial, you'll implement a shallow neural network to classify digits ranging from 0 to 9. The dataset you'll use is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/. A French guy put it up, he's very famous in the DL comunity, he's called Yann Lecun and is now both head of the Facebook AI reseach program and head of something in the University of New York...\n",
    "\n",
    "\n",
    "###Â First step\n",
    "\n",
    "As a first step, I invite you to discover what is MNIST. You might find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) to be usefull, but feel to browse the web.\n",
    "\n",
    "Once you get the idea, you can download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset in this directory (does that work on Windows OS ?)\n",
    "! wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle, gzip, numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def to_one_hot(y, n_classes=10):\n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "X_train, y_train = train_set[0], to_one_hot(train_set[1], 10)\n",
    "X_valid, y_valid = valid_set[0], to_one_hot(valid_set[1], 10)\n",
    "X_test,  y_test  = test_set[0],  to_one_hot(test_set[1], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can now implement a 2 layers NN\n",
    "\n",
    "Now that you have the data, you can build the a shallow neural network (SNN). I expect your SNN to have two layers. \n",
    "    - Layer 1 has 20 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood (wich is also the cross entropy)\n",
    "    \n",
    "You'll need to comment your work such that I understand that you understand what you are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Define the variables here (initialize the weights with the np.random.normal module):\n",
    "\n",
    "# 2- Define the model here (with loss):\n",
    "\n",
    "# 3- Define derivatives here (maybe...):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train you model\n",
    "\n",
    "You may use Standard Gradient Descent (SGD) to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus point :\n",
    "Experiment with other activation function and weight initializer. What do you observe ?  \n",
    "Activation examples : Tanh, Relu, Leaky Relu, Selu, Gated activation..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
